First developed n a 2017 paper called attention is all you need

When the tokens are first created theyre are merly lookup table with no reference to context 

Its only in the next step that the surrounding embeddigs have the ability to pass info to each other making them more richer 

A well trained attention block calculates how much you need move this genrnic embedding to a diretion which is more suitable to the context 


